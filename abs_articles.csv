Text,Abstract,Conclusion
"Cancer disease has been reported as one of the deadliest genetic maladies of the human genome. It has been the research interest until today by doctors, pathologist, biologist and others life science and health professionals. The World Health Organization (WHO) reported cancer disease having 14 million new cases in 2012. This disease is a major cause or morbidity and mortality that accounts of death globaly resulting 8.8 million deaths in 2015 [1]. The World Cancer Report described cancer as a global problem and projected an increase to 20 million new cases by 2025 [2]. There are several research in cancer classification that used different techniques using gene analysis and classification. The important goal in cancer disease research is to identify the specific genes that caused normal cells to mutate into cancer disease. There are approaches in technology that reveals the cellular and molecular level of cancer. In a cancer disease sample such a cell biopsy to be processed, thousands of genes at a time can be subjected for analysis in a single chip called microarray. Microarrays are microscopic slides that contain ordered series of samples of DNA (Deoxyribonucleic acids), RNA (Ribonucleic acids), protein, or tissue and others [3]. A single chip microarray can measure the gene expressions of 30,000 gene sample that represents most of the human genome [4]. The challenge of cancer classification using the microarray is the application of model based selection and prediction algorithm that will classify the cancer genes using gene expression data. The computation time, classification accuracy, and its biological relevance in the cancer classification was still in question. The main goal of this study is to explore and analyze the published papers in cancer classification. The scope of this paper is to present the cancer classification using machine learning models.
This paper mainly explored and analyzed published papers about the topic. The papers were extracted from Scopus and Classification Multi Strategy Biological Scalability method class Evaluation meaning Support No Good Vector No Max-Margin Machine Boosting Yes Max-Margin Yes Class dependent Decision Tree Yes Entropy Yes Good function K-Nearest Yes Similarity No Not scalable Neighbor Cluster–based No Not scalable Similarity Yes Similarity Tuple Gene No Weighted Yes Fair Selection voting Fisher Linear Discriminant No Fair Discriminant Yes Analysis Analysis Neural Yes Perceptron No Fair Network Naïve Bayes Yes Distribution No Fair modeling PubMed and analyzed using Cited Reference Explorer (CRExplorer) in section A. To illustrate the tasks for the classifier model, the following activities were described in the next sections. Describing the dataset in Section B. The application of machine learning cancer classifier method in Section C. A. Papers unde study The reference publication study used on this paper was extracted from PubMed [6] and Scopus [11]. The research continued to gain interest from researchers of cancer classifiers and gene expression analysis methods since 1995. Refer to Figure 1 for the Scopus result for search analysis documents per year source from 2000-2017 period. This shows that the topic of interest has still the traction each year. The reference publication study used on this paper was extracted from PubMed [6] and Scopus [11]. The research continued to gain interest from researchers of cancer classifiers and gene expression analysis methods since 1995. Refer to Figure 1 for the Scopus result for search analysis documents per year source from 2000-2017 period. This shows that the topic of interest has still the traction each year. The Cited References Explorer (CRExplorer) [12] is another tool that can analyze the historical roots of fields, topics, or researches. The method used for the analysis based on RPYS is the frequency of references cited in the researches in terms of the publication years. The RPYS analysis of the topic. There were 55,040 cited references in the 1967-2017 period and 51 different citing publications years with the total of 1448 publications cited. The cited publication years considered were from 2000-2017 period with 18 publication year. The researchers on cancer classification using gene expression started its traction on 1995 with its influential work on 1999 of [13,17,19, 43,44,45]. The peak of the research on 2000 with the use of microarray more datasets and the evaluation using classification methods. The work of [45] marked its constant cited reference of the citing years thereafter. The evaluations and classification method were discussed. B. Dataset The dataset used is a microarray from the Golub experiment [13]. The dataset is available online from the repository of Stanford Hastie CASI files [46]. The process of obtaining the best model for predicting disease classes from a given raw data set collected by scanning genetic microarrays from 72 patients, each suffering from one of the strains of cancer (leukemia). For each of the patients, the scanner tabulated the values of 7129 genes, each of which was assigned a numerical value. The dataset is 65% ALL and 35% for AML. It contains the gene description and gene accession number. The dataset has training set of 7129 rows (instances) with 72 columns (features) with 38 samples. The test set contains 6627 unique values of genes from the 7129 values with 34 samples. The data was normalized for this experiment to select the genes that was correlated to the outcome of the combined features. This reduces the genes and to increase the classification accuracy. Using linear method T-values, this reduced the number of genes for the training model. Using the formula below: ( Av1 − Av2 ) T-test for mean difference = (1) ( o1 2 / N1 + o 2 2 / N2 ) Av1 and Av2 average of 1 class from the given gene expression classes. The sigma are the standard deviation of each classes. Then, N1 and N2 are samples whose class has T-values. and does not have the T-values. Then running the experiment using selected machine learning tool for cancer classification method. C. Machine Learning Tool for Cancer Classification Model Using the Google Colaboratory (CO) [47] notebook with python programming to run the experiment for the AML and ALL dataset. Using the PyML package, that has classification and regression methods. SVM is a classifier PyML. The classifier used in the experiment were Support Vector Machine (SVM) and Boosting (Extreme Gradient Boosting). SVM is a supervised learning method that analyzed data for classification.",The World Cancer Report described cancer as a global problem because it affects the whole greater population. There will be a projected increase to 20 million new cases by 2025 [2]. There are several known published literatures on cancer classification techniques with varying models and implementations. This paper presents the existing technology of microarray gene expression and classify the cancer genes using machine learning algorithms. A logical design was presented using supervised classification and gene selection model. This model can improve the process and method of identifying and classifying cancer disease.,"This paper presented the gene expression data analysis and classify the results based on the cancer dataset. The literature analysis presented cancer classification using gene expression was still a topic on interest by most researchers in this field. The survey analysis of the cancer classification model evaluation Figure 2. Logical model for supervised learning [53] presented the advantage and disadvantages of each. Gene selection is an important phase in the preprocessing and cancer classification. This paper demonstrated the cancer classification model and applied the SVM and Boosting that provide insights of their application in the gene expression data. The performance and accuracy reported having 58% and 64% can indicate that the experiment can be improved and comparable to other results of published literatures. The next step is to investigate the cancer classification techniques specific to cancer genome or type of cancer disease using the other machine learning and deep learning techniques. Python packages can be programmed based on the model presented will be the next step. The application of artificial intelligence can be considered as the next step as our research in cancer classification in histology, digital oncology and pathology."
"Distracted driving can be seen in some situations where-in the driver diverts his/her attention from activities critical for safe driving [1] and in some situations, this could potentially cause road accidents which are most likely to occur in traffic congested areas. Cognitive, manual, and visual distractions are some factors that affect the driving capabilities of a driver when exposed to prolonged driving [2]. Visual distraction in the context of driving can be determined as activities that take the driver’s eye off the road, except for activities that are necessary in driving such as observing other vehicles or checking side mirrors [3]. The National Highway Traffic Safety Administration (NHTSA) of the U.S.A. has provided guidelines for the public wherein it states that a task that diverts the driver’s visual attention from the road should not exceed 2 seconds [4]. This guideline was based on their previous research of the impact that driver inattention has on near-crash or crash risks and it is said that drivers involved in crashes had an average total time of eyes off the road of nearly 2 seconds [5][6]. Aside from visual distraction, conversations held while driving take an amount of attention away from the task at hand. By assessing the results of the National Motor Vehicle Crash Causation Survey (NMVCCS), it has been found that conversing with a passenger was the top factor coming from an internal source that was present in the assessed crashes. In hopes of contributing to the ongoing research on developing modern and more accurate models for driver-state monitoring systems, this paper presents a non-intrusive camera-based approach which was developed and implemented to detect visual distraction cues [7] and cues that can be detected visually (talking). Transport Network Vehicle Service (TNVS) drivers are the primary focus of this study since they play a role in everyday commute, putting the passenger’s safety in their hands whenever they choose to use their services. It also provides new knowledge regarding the presence and frequency of distraction during a usual transit of a TNVS driver [8]. Detecting driver distraction relies heavily on analyzing visual data and computer vision concepts. The methods that were implemented in this study were carefully chosen to coincide with the area of inquiry. This paper presents a model that implements a non-intrusive camera-based approach for detecting a Grab driver's lapse indicators. Figure 4 depicts the design of the model. The recorded footage of the driver’s face during transit should undergo through processes to detect the lapse indicators. Visual data was gathered through recording the faces of the Grab drivers during transit. Twenty (20) respondents took part in the experiment. The training dataset is made up of nineteen (19) driver values, and the remaining one was left for testing. Each video file was pre-processed by having them segmented into parts wherein actual driving occurs. This means that traffic footage was discarded and unused for testing. The videos were rendered at twenty-five (25) frames per second (fps, making the 2- second margin a constant of fifty (50) frames. Furthermore, the video data was muted and turned into grayscale. A sample of one hundred-twenty (120) seconds was taken from each video. This is divided into two equal subsets, one for eye gaze and the other for speaking. The sixty (60) second subset is made up of thirty (30) seconds of pure distraction, and 30 seconds of non-distracted driving footage. The symmetry is done to avoid the data becoming biased towards one prediction. The training set consists of a total of fifty-seven thousand (57,000) values (2,280 seconds worth). For both speaking and eye gaze, each driver set has one thousand 114 five hundred (1,500) frames, measuring at about one (1) minute. The system does not deliver real-time detection of distraction as indicated by the processing of the video file after it has been recorded. It was not designed with the intention of providing a real-time detection machine, but to assess the capability of the designed methodology. Once pre-processing is done, the individual frames of the video will be extracted for the facial attributes necessary. Eye Gaze Tracking focuses on the eye movements and the accompanying head pose that signifies glances to assess whether the driver has not been in clear view of the road for a pre-determined sequence of frames. Computer vision concepts were applied to the model which assesses with a binary classification of whether the driver is distracted (value = 1) or not (value = 0). The classifications were compared to the perceptual evaluations to account for whether the model had an accurate detection capability of distractions. The model’s detecting power is determined by a confusion matrix and an evaluation of the model’s F-measure. All of which are based on perceptual evaluation that will serve as the baseline of the action as a distraction or not. The data were then partitioned into subsets with 19 driver data sets serving as training data, and the remaining 1 driver data set as the testing data. The testing data was not included in the training whatsoever. The concepts introduced and used in [8] were implemented in this study where it is assumed that head movements relate to where the driver is looking at. Figure 5 depicts the usual position of a Grab driver. A scenario wherein the driver’s eyes were off the road is not automatically marked by the model because a glimpse does not automatically indicate visual distraction, a continuous 2 seconds of distraction must occur. However, returning gaze to the forward road also does not automatically signify a non-distracted event. The 2 second rule was used as the baseline for identifying if the driver was not focused on the road. This means that the driver will remain to be considered as distracted even after returning to the forward road, until the specified frame interval has been met. For a 25-frames-per-second rendering, the driver must remain undistracted for 17 frames as observed in the values for the detection to change from distracted to non-distracted. The same rules were applied for speaking detection as no formal rules have been established for it at the time of the research. The recorded footage was processed frame by frame to extract facial landmarks, eye gaze, and action units from the driver. The process of detecting distraction was conducted through gaze tracking, head pose orientation, Action Unit detection, 2D facial landmarks, and 3D facial landmarks. OpenFace, as depicted in Figure 6, was used to determine the facial attributes of the driver. The OpenFace software is a facial behavior analysis toolkit developed through an integrated implementation of the Multicomp Group, Technologies Institute at the Carnegie Mellon University and Rainbow Group, Computer Laboratory, University of Cambridge [19]. This toolkit contains a feature of facial action unit recognition, facial landmarks tracking, head pose tracking, and gaze tracking can output a csv file that is used for identifying distraction features. The columns of gaze_angle_x, gaze_angle_y, x_51, y_51, X_51, Y_51, Z_51, AU23_r, AU23_c, AU25_r, and AU25_c were used to determine the approximate gaze position where the driver is looking at as well as the mouth movements exhibited by the driver. These columns were obtained from the output of OpenFace. The two (2) second timer functions as a separate existing counter outside of the timestamps of the video. A detected occurrence of distraction signifies the start of the counting. Since the videos were rendered at 25fps, two seconds (50 frames) of distraction in succession was needed to detect distraction. However, the presence of a non-distracted frame does not automatically reset the timer. As per conducted on the research 17 frames of continuous non-distraction was needed to reset the timer. Certain events such as moving back and forth from the road and a cell phone do not constitute a continuous distraction frameset. However, it is still distraction. The 17-frame buffer allows the model to detect these distractions as well. Mouth movements were recognized based on tracking points on the mouth that signify frequent openings and closure in succession. With the help of OpenFace, facial landmarks in the 2D and 2D plane were used to map the mouth points location to indicate movement. As with the eye gaze, mouth movements were also applied with the same algorithm of the 2-second rule. This was done since there was no specific time restriction placed upon the mouth movement by a previous research. 3.2 Binary Classification To assure the accurate distraction detection of the model, a baseline was established. The driving videos that were preprocessed beforehand were subjected to perceptual evaluation of driver videos to manually assess if the driver is distracted by indicating the second timeframe that the driver is distracted. This manual annotation served as the baseline for the model’s power in detecting distraction indicators [20]. The annotation was used as a 115 comparison to the model’s output. Meanwhile stratified K-Fold method was used for cross validation. Binary classification with values of 0 as not distracted and 1 as distracted will sort out the detected features into which ones are classified as possible distraction cues. k-nearest Neighbor Classifier (k-NN) was used as the classifier [21] [22]. k-NN was implemented using Python version 3.6. Libraries of numpy, sklearn, pandas, and csv were utilized in the development of the model for detection. Numpy was used for array creation needed to hold the values that were stored inside .csv files. Sklearn contained the cross-validation and k-NN functions. Pandas were used in the writing and creation of new files storing the output detection (raw) files that the model made. Lastly, Csv was used for .csv file manipulation. Upon classification, determining the detection power of the model was based on the manual annotations and evaluations of the perceptual evaluators. For comparison, four outputs will be determined: True Positive detection, True Negative detection, False Negative detection, and False Positive detection. These were placed all in a confusion matrix in order to quantify the extracted results. Depicted in Figure 7 is the manual annotation by the evaluators. The outputs of the classifier were then inputted to an algorithm designed to sift through the data and apply a timer that indicates distraction for a consecutive of 50 frames, amounting to 2 seconds on a 25-fps video. The algorithm also clusters the output data into blocks of 1 second (25 frames) each and unifies the data within that cluster by majority. If a majority of the cluster is predicted as 1, then the whole second assimilates a value of 1, and vice versa. This clustering allows proper matching to account for the imprecision of the manual annotation, as well as preventing random data to be classified as distracted when it is not (outliers). This is done separately for speaking and gaze outputs. Both results are then consolidated by an “OR” function to output the final results of the detection model. 3.3 Model Validation Binary classification determines a selection as either positive or negative. However, comparing the model’s output to actual data in training can have one of four outcomes. First is True Positive detection which involves a positive match between the detected feature, and the manually annotated video that was evaluated. A high number of this output will determine the quality of the model in detecting distractions. A low accuracy would mean that the formulated model is not suitable for detecting distraction cues in the context with which the study was conducted. A True Negative detection on the other hand signifies a point in the video wherein it was not annotated to be a distraction, and the model does not detect this as well. False positive detection would indicate that the model has detected a distraction cue wherein there was no distraction present as indicated by the perceptual evaluators. False negative detection would indicate that the model has not detected a distraction cue but the perceptual evaluators have annotated the segment to be containing a distraction. In analyzing the data, Accuracy, Recall, and Precision were the methods used for final validation of the model in training [21] [22]. The F-measure is the harmonic mean of Precision and Recall [23]. This is done to quantify or give a numerical score to the predictive power of the model. A score of 1 (100%) denotes a perfect classification procedure for the model. Meanwhile, 0 denotes the worst F-measure. For clarification, the result of this score does not coincide with the results of accuracy. However, Fmeasure allows for a better classification of the predictive power of the model instead of just assessing how much the model got right. Once the model has achieved an acceptable F-score to assess its validity, the testing phase process begins. In line with this, the model was subjected to assess whether it is the right fit for the sample data obtained. This is done so in order to generalize the performance of the model in real world values. The model cannot be taught 100% cases that exhibit distraction. By implementing Kfold method for validating the data, it can be estimated how well a model can perform in implementation on values that it was not taught. The K-Fold Method is a cross validation methodology that partitions the data set into k equal subsets. One of the subsets will be allocated for testing, while the rest for training the model. The process is repeated k times, with each time making use of a different subset as the testing data. The results of these tests will be averaged in order to produce the output of the process. This results in a better accuracy by compute time will also be increased due to the repetitive action. The driver set used for testing the data has a video length of 7 minutes, 46 seconds, and 3 frames (11,653 frames). The distracted portions of this video amounts to 9,481 frames while the nondistracted portions of the video totals to 2,172 frames. The model peaked at 95% (0.9475) F-measure with 30 as the value of k for the classifier as presented in Figure 8. This specific driver set was the set chosen as training data as a result of the K-Fold Validation. This model produced the highest F-measure score in comparison to the other data sets. Factors attributing to this are as follows: best lit recording, numerous samples of varying distraction occurrences, driver speaking to the passenger, face is in clear view of the camera, and sudden bursts of extremely dark or extremely light frames were not common. There are three (3) main features of the detection prototype that were individually assessed for their F-Measure score. Namely:  Raw values produced by k-Nearest Neighbor Classifier for eye gaze, action units, and facial landmarks (Feature A);  Raw values prosduced by k-Nearest Neighbor Classifier exclusively for eye gaze (Feature B); and lastly, 116  k-Nearest Neighbor Classifier Outputs Processed through the time restriction and value clustering algorithm (Feature C). Feature C presented the highest F-Measure (95%). The data presented in the succeeding pages reflect the results gathered from this specific feature. The graph presented in Figure 8 was rendered by the model to indicate the spiking accuracy of the raw detection (before application of time restriction algorithms) at 80%. The spiking graph was caused by the outliers present in the video inputs caused by varying lighting conditions resulting in poorer values outputted by the OpenFace feature extraction. After running through the time restriction algorithm and value clustering, the model reached 0.9475 (95%) at 30 neighbor values for F-Measure. The average of the statistics based on a Confusion Matrix made for each of the testing criteria are presented in Table 1. The data presented in Table 2 displays a close margin between the actual and predicted values. From the confusion matrix presented, the predicting power of the model can be quantified. The model predicted 379.24 seconds to be distracted, and 86.88 seconds to be non-distracted. This amounts to a 1.55 % and 6.76% percentage error for the distracted and non-distracted predictions respectively. The average of the statistics based on a Confusion Matrix made for each of the testing criteria are presented in Table 3. The results from this study signify that the combination of facial landmarks, eye gaze, and action units provide a detection power of 95% for the specific data that was tested on.","Factors related to distraction have been one of the major causes of accidents related to driving. This paper presents a model that detects distraction cues of a Grab driver that mainly focuses on visual distraction and continuous communication indicated by mouth movements. The data gathered was in the form of a video footage obtained from each transit recorded. Each video was processed frame by frame to extract necessary features for detecting distraction cues through OpenFace. Binary classification was used to assess whether the driver is distracted or not. To determine the predictive power of the model, K-fold cross validation method was used. Since Grab has been widely used in the Philippines, the findings would allow for interesting knowledge regarding the behavior that these drivers exhibited in varying situations with regard to distraction.","The study presents proof that eye gaze in relation to the driver’s head pose, action units, and facial landmarks on the lips signify a viable parameter for use in detecting distraction cues. These attributes provide sufficient information to create a model with a 95% detection power. This model feature (Feature C) produces the highest detection power out of the three considered model features. It combined the raw outputs of the k-NN classifier and subjected into a clustering and time restriction algorithm. The ceiling at 95% detection power (F-measure) can be attributed to human errors associated in the annotation of the video. In addition, the model’s detection power is limited by the ability of the feature extraction software (OpenFace) to accurately determine the values of the driver’s face. Positions wherein the driver’s face is completely facing the side returns invalid values, rendering the prediction model unable to classify such an occurrence. Furthermore, drivers whose facial movements are very minute and limited decrease the accuracy of the model since it relies on distinguishable eye movements and head pose orientations. Future work on improving the model could be through triggering specific instances of distraction for the model to be trained accordingly. Factors such as light and shadow in extreme values hinder the detection capability. A higher quality camera could be used for a more accurate detection of facial features. Different frame rates in rendering the output video could be used to see if there are changes in the detection power if frame rates are reduced or increased. A different feature extraction software or toolkit could be used to extract facial values to see the difference in the capabilities of the two models. The inclusion of auditory cues is an extension possibility of this model since it currently only focuses on what is visual. Integrating these could create a model that yields better results."
"The quality management system (QMS) is considered as an organizational structured that consists of quality procedures, processes and resources which is setup systematically to achieve quality objectives. Today, State Universities and Colleges decided to implement the principles and methodologies of the quality management system for two main reasons. First, is to improve their operations and increase customer’s satisfactions. Second is their wish for formal international recognition to drive corporate reputation. [1] In this paper, the researchers identified the current challenges of the quality assurance department of the academic sectors. First, their quality assurance staff is doing manual classification of digital documents such as audit findings and reports. This normally results to the delay of data analysis and late submission of management reports to the top management. In addition, it creates a bottleneck for a large data to be analyzed in a short period of time and this becomes a challenge for the administrative sectors. Their main tasks are to create reports punctually and provide Revised Manuscript Received on July 12, 2019. Ken Jon M. Tarnate, Mapua University, Manila, Philippines Madhavi Devaraj, Mapua University, Manila, Philippines adequate and latest data for the top management to make critical decisions. Second, most auditors during quality audits and management reviews had difficulty on identifying and categorizing the audit findings according to its major clauses of the ISO 9001:2015 QMS Requirements. And this is where automatic text categorization fits to solve the problem of the quality assurance auditing team of the academic institutions. However, the text categorization process is also facing a challenge, in terms of picking the right “word representation model” for the traditional machine learning algorithms. [15] For this purposed, the researchers developed a deep neural networks models with a combine word representation models (word encoding plus an embedding dimension layer) to classify an audit reports according to its major clauses of the ISO 9001:2015 QMS Requirement. The authors built a two layered LSTM and Bi-LSTM neural networks with a total number of 225 hidden units and compare the performance of those models to the traditional LSTM and Bi-LSTM models.The main contributions of this paper are the development of machine learning model to help the internal auditors to identify and classify an audit reports according to the major clause of the ISO 9001:2015 QMS Requirements. Second, the researchers investigated the learning ability of the four recurrent neural network models using a combined word representation models (word encoding with an embedding dimension layer). And most importantly, the authors able to speed up the process of the extractions and generations of management reports based from the results of the internal and external quality audits conducted in the university. In the Exploratory Data Analysis, the researchers used the RStudio IDE version 1.1463. Using the “R” programming language and libraries such as “ggplot2”, “dplyr”, “quanteda”, “caret” and “doSnow” using these methods the authors able to analyze and graph the dataset. in a sophisticated way using the visualization techniques in data mining. Meanwhile, the researchers also used the MATLAB Software version 2018b in doing the entire standard procedures of the text categorization process of audit reports according to the ISO 9001:2015 QMS Requirements. This figure shows the visual representation of the categorization process of audit reports; Data gathering and extraction, Exploratory Data Analysis, Data Pre-processing, Training, Validation and Testing the RNN(s) Model and Evaluation of RNN Models. The datasets were extracted from the Quality Assurance Databases on one of the State University in the Philippines which their quality management system is ISO certified. The entire datasets were composed of 2583 audit reports. Each audit reports have a labeled according to its major clauses and specific clauses, status and its types of nonconformities; major nonconformance (NC), minor (MiN), and opportunity for improvement (OFI). Using the RStudio IDE version 1.1.463. The authors explored the data of the quality assurance department of the higher education institutions. Using the data visualization techniques, it helped the top management to foresee, on which principles and methodologies of the ISO 9001:2015 QMS Requirements they are lacking with. Through this exploratory data analysis, the top management of the institution had been able to identify which areas of the management are needed to be prioritized. Table I. Shows the breakdown of the entire datasets according to its major clauses. Basically, during quality audits, the auditors are looking for this major clause of the ISO 9001:2015 QMS Requirements. It shows that the area of “Support” has the greatest number of findings. This means that the current Quality Management System of the institution is lacking on the “Support Systems”, this also means, that their current weakness is in the area of support groups and programs. However, the lowest number of findings can be found in the area of “Improvement” this can be means that the current QMS of the institution, is stable enough to continue on the day to day operations. In this stage, the researchers cleaned and normalized the text data and converted it into a computable format so that the deep learning model can process and train by it. The researchers tokenized first the documents, then removed all the stopwords, punctuations and lowercase all the characters. Then converted the unstructured text data into a structured data; this can be done by using a word representation model. The authors used the “doc2sequence” n-gram model to convert the text data into vectors. Then reduced the dimensionality of the data by assigning a fixed number or length of attributes through word embedding. After the data preparations, the researchers divided the datasets into to three states, 70% for the training of the model, 15% for the validation of the model and another 15% for the testing of RNN(s) the model. Categorization of ISO 9001:2015 Audit Reports Using Recurrent Neural Networks 1776 Published By: Blue Eyes Intelligence Engineering & Sciences Publication Retrieval Number B1018078219/19©BEIESP DOI: 10.35940/ijrte.B1018.078219 Then the authors fed the data in the recurrent neural networks model to train and learn from it. In the experiment a fully dense connected network was used and each RNN models have 225 hidden units’ layers and have an embedding dimension of 100. The authors used the “Adam” optimization solver to iteratively update the network weights during training then used the “SoftMax” activation functions to normalize the input values to outputs vectors and assigned probability distributions on the output values. And to avoid over fitting on the training data, the researchers used the “dropout” regularization technique for the Deep-LSTM and Deep-Bidirectional-LSTM. This figure shows the actual “R” code to generate a certain report needed by the quality assurance department. This code able to summarize the results of the internal and external quality audits findings. It also able to graphs and calculates the average nonconformities and status of the findings. In this figure, based from automated extraction process of audit reports, there are 1,350 or 52% of audit findings are still open or not resolve, however there are 1224 or 47% of audit findings are already resolved. Meanwhile there nine (9) or 1% of audit findings is undetermined. In this figure, based from the data mining process, the system identified the count of nonconformities of the entire university on the ISO 9001 Standard. According to these results, there are 1278 processes that are major nonconformance on the ISO 9001 QMS standard and 873 minor findings. However, there are 405 findings which can be a good opportunity to improve for the entire QMS process of the university. Lastly, some auditors submitted 15 compliance and 12 positive audit findings. It indicates that there are existing processes that needs to be maintained and retained because it causes positive impact on the university. The authors evaluated the model by getting its classification accuracy with respect to its cross-entropy loss. Table II. Shows the classification accuracy of each RNN(s) Model. Based from these results, the Deep-Bidirectional LSTM outperformed the other three RNN(s) model. Table III. Shows the actual predictions of the Deep-Bidirectional LSTM. The model predicted nine (9) correct major clauses out of 10 trials.","The Quality Assurance Department of the educational sectors is rapidly generating digital documents. The continuous increase of digital documents may become a risk and challenge in the future. Interpreting and analyzing those digital data in a short period of time is very critical and crucial for the top management to support their decisions. By this purpose, this paper explored the possibility of machine learning and data mining process to improve the Quality Assurance Management System process, specifically in the Quality Audit procedures and generation of management reports. The researchers developed a machine learning model that predicts an audit report according to the major clauses of the ISO 9001:2015 Quality Management System (QMS) Requirements. The proposed data mining process helps the top management to identify which principles of the ISO 9001:2015 QMS Requirements they are lacking. The authors used four different Recurrent Neural Networks (RNNs) as a classifier; (1) Long Short Term-Memory (LSTM), (2) Bidirectional-LSTM, (3) Deep-LSTM and a (4) Deep-Bidirectional-LSTM Recurrent Neural Networks with a combine word representation models (word encoding plus an embedding dimension layer). The Deep-Bidirectional-LSTM outperformed the other three RNN models. Where it achieved an average classification accuracy of 91.10%","In general, the proposed deep learning model helps the internal auditors of the university. They used the machine learning model as a cross reference and as a validation tool for labelling an audit findings or reports. In addition, the proposed data mining and machine learning framework helps the administrative staffs of the quality assurance department to generate reports in a short period of time. And the top management had been able to receive an updated and adequate data where their can based and support their decisions from the results of the data mining process. Lastly, the Deep-Bidirectional LSTM outperformed the other three RNN(s) model having an average classification accuracy of 91.10% However, during the validation stage; the traditional LSTM has the highest validation accuracy and outperformed the two layered Deep-LSTM Model having an average classification accuracy of 90.2%. This signifies that the adding a neural network layer has no significant factor in the increase of the accuracy of the deep neural networks. This is also due to the vanishing gradient descent problem of the deep neural networks. For further studies, researchers should also consider and look on this problem when using a deep learning model for a classification task. By this means, as advice for the future researchers, they should carefully examined the hyper parameters, dropouts and hidden units before adding a neural network layer when building a deep neural networks model."
